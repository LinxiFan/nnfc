#!/usr/bin/env python3

import torch
import torch.nn.functional as F
from torch.autograd import Variable

import resnet_models

import h5py
import numpy as np
import os
import six
import subprocess as sub

TMPDIR = os.environ['NNFC_TEST_TMPDIR']

batch_size = 1
num_channels = 3
height = 32
width = 32
inputs = Variable(torch.randn(batch_size, num_channels, height, width))
print('inputs.shape', inputs.shape)

net = resnet_models.TinyNet()
#net = resnet_models.ResNet18()
model_params = net.state_dict()

for param_name in model_params.keys():
    shape = model_params[param_name].size()
    model_params[param_name] = torch.randn(shape)

net.eval()
    
hdf5_file = os.path.join(TMPDIR, 'simplecnn.h5')

with h5py.File(hdf5_file, 'w') as f:
    for param_name in model_params.keys():
        f.create_dataset(param_name, data=model_params[param_name])
            
    # layer 1
    # f.create_dataset('conv1.zero_padding', [1], dtype=np.uint64)
    # f.create_dataset('conv1.stride', [1], dtype=np.uint64)
    # f.create_dataset('bn1.eps', [0.00001], dtype=np.float32)

    f.create_dataset('hdf5_version', data=six.u(h5py.version.hdf5_version))

    # # layers 2 and 3
    # f.create_dataset('layer1.0.conv1.zero_padding', np.asarray([1]))
    # f.create_dataset('layer1.0.conv1.stride', np.asarray([1]))
    # f.create_dataset('layer1.0.conv2.zero_padding', np.asarray([1]))
    # f.create_dataset('layer1.0.conv2.stride', np.asarray([1]))

    # # layers 4 and 5
    # f.create_dataset('layer1.1.conv1.zero_padding', np.asarray([1]))
    # f.create_dataset('layer1.1.conv1.stride', np.asarray([1]))
    # f.create_dataset('layer1.1.conv2.zero_padding', np.asarray([1]))
    # f.create_dataset('layer1.1.conv2.stride', np.asarray([1]))

    # # layers 6 and 7
    # f.create_dataset('layer2.0.conv1.zero_padding', np.asarray([1]))
    # f.create_dataset('layer2.0.conv1.stride', np.asarray([2]))
    # f.create_dataset('layer2.0.conv2.zero_padding', np.asarray([1]))
    # f.create_dataset('layer2.0.conv2.stride', np.asarray([1]))

    # # layers 8 and 9
    # f.create_dataset('layer2.1.conv1.zero_padding', np.asarray([1]))
    # f.create_dataset('layer2.1.conv1.stride', np.asarray([1]))
    # f.create_dataset('layer2.1.conv2.zero_padding', np.asarray([1]))
    # f.create_dataset('layer2.1.conv2.stride', np.asarray([1]))

    # # layers 10 and 11
    # f.create_dataset('layer3.0.conv1.zero_padding', np.asarray([1]))
    # f.create_dataset('layer3.0.conv1.stride', np.asarray([2]))
    # f.create_dataset('layer3.0.conv2.zero_padding', np.asarray([1]))
    # f.create_dataset('layer3.0.conv2.stride', np.asarray([1]))
    
    # # layers 12 and 13
    # f.create_dataset('layer3.1.conv1.zero_padding', np.asarray([1]))
    # f.create_dataset('layer3.1.conv1.stride', np.asarray([1]))
    # f.create_dataset('layer3.1.conv2.zero_padding', np.asarray([1]))
    # f.create_dataset('layer3.1.conv2.stride', np.asarray([1]))

    # # layers 14 and 15
    # f.create_dataset('layer4.0.conv1.zero_padding', np.asarray([1]))
    # f.create_dataset('layer4.0.conv1.stride', np.asarray([2]))
    # f.create_dataset('layer4.0.conv2.zero_padding', np.asarray([1]))
    # f.create_dataset('layer4.0.conv2.stride', np.asarray([1]))
    
    # # layers 16 and 17
    # f.create_dataset('layer4.1.conv1.zero_padding', np.asarray([1]))
    # f.create_dataset('layer4.1.conv1.stride', np.asarray([1]))
    # f.create_dataset('layer4.1.conv2.zero_padding', np.asarray([1]))
    # f.create_dataset('layer4.1.conv2.stride', np.asarray([1]))

    outputs = net(inputs)
    f.create_dataset('input', data=inputs.data.numpy())
    f.create_dataset('output', data=outputs.data.numpy())
    
sub.check_call(['./simplecnn.bin', hdf5_file])
